{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mndata = MNIST('../HW1/Dataset')\n",
    "trainX = np.array(mndata.load_training()[0])[:50000]\n",
    "trainY = np.array(mndata.load_training()[1])[:50000]\n",
    "\n",
    "testX = np.array(mndata.load_testing()[0])[:1000]\n",
    "testY = np.array(mndata.load_testing()[1])[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valIndices = np.random.choice(len(trainX), 2000)\n",
    "nonValIndices = [x for x in range(len(trainX)) if x not in valIndices]\n",
    "\n",
    "valX = trainX[valIndices]\n",
    "valY = trainY[valIndices]\n",
    "\n",
    "trainX = trainX[nonValIndices]\n",
    "trainY = trainY[nonValIndices]\n",
    "\n",
    "testX = np.array(mndata.load_testing()[0])[:2000]\n",
    "testY = np.array(mndata.load_testing()[1])[:2000]\n",
    "\n",
    "def feat(data,i):\n",
    "    return data[i].tolist()\n",
    "\n",
    "def oneHot(clas, noOfClasses):\n",
    "    feat = np.zeros(noOfClasses)\n",
    "    feat[clas] = 1;\n",
    "    return feat\n",
    "\n",
    "trnX = np.array([feat(trainX,i) for i in range(trainX.shape[0])])/256.0\n",
    "trnY = np.array([oneHot(trainY[i], 10) for i in range(trainX.shape[0])])\n",
    "\n",
    "tstX = np.array([feat(testX,i) for i in range(testX.shape[0])])/256.0\n",
    "tstY = np.array([oneHot(testY[i], 10) for i in range(testX.shape[0])])\n",
    "\n",
    "valX = np.array([feat(valX,i) for i in range(valX.shape[0])])/256.0\n",
    "valY = np.array([oneHot(valY[i], 10) for i in range(valX.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trnY[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.7\n",
      "12.5\n",
      "14.15\n",
      "16.55\n",
      "18.9\n",
      "21.2\n",
      "23.4\n",
      "25.7\n",
      "28.35\n",
      "30.85\n",
      "32.6\n",
      "34.1\n",
      "35.25\n",
      "37.05\n",
      "38.45\n",
      "39.65\n",
      "40.7\n",
      "41.75\n",
      "42.75\n",
      "43.8\n",
      "44.7\n",
      "45.7\n",
      "46.5\n",
      "47.6\n",
      "48.3\n",
      "49.1\n",
      "49.55\n",
      "50.4\n",
      "51.0\n",
      "51.4\n",
      "52.05\n",
      "52.6\n",
      "53.0\n",
      "53.7\n",
      "53.9\n",
      "54.4\n",
      "54.85\n",
      "55.5\n",
      "55.95\n",
      "56.4\n",
      "56.5\n",
      "56.95\n",
      "57.1\n",
      "57.55\n",
      "57.75\n",
      "58.1\n",
      "58.45\n",
      "58.8\n",
      "59.15\n",
      "59.1\n",
      "59.75\n",
      "60.1\n",
      "60.35\n",
      "60.85\n",
      "61.15\n",
      "61.35\n",
      "61.65\n",
      "61.85\n",
      "62.05\n",
      "62.15\n",
      "62.45\n",
      "62.65\n",
      "62.65\n",
      "62.75\n",
      "63.05\n",
      "63.1\n",
      "63.25\n",
      "63.6\n",
      "63.95\n",
      "63.85\n",
      "64.1\n",
      "64.2\n",
      "64.4\n",
      "64.55\n",
      "64.8\n",
      "64.95\n",
      "65.05\n",
      "65.05\n",
      "64.95\n",
      "65.1\n",
      "65.1\n",
      "65.2\n",
      "65.25\n",
      "65.35\n",
      "65.5\n",
      "65.55\n",
      "65.65\n",
      "65.75\n",
      "65.75\n",
      "65.8\n",
      "66.05\n",
      "66.0\n",
      "66.1\n",
      "66.3\n",
      "66.4\n",
      "66.65\n",
      "66.65\n",
      "66.75\n",
      "66.65\n",
      "66.85\n",
      "66.8\n",
      "67.1\n",
      "67.15\n",
      "67.1\n",
      "67.2\n",
      "67.25\n",
      "67.4\n",
      "67.55\n",
      "67.45\n",
      "67.65\n",
      "67.7\n",
      "67.85\n",
      "67.9\n",
      "68.05\n",
      "68.1\n",
      "68.2\n",
      "68.3\n",
      "68.3\n",
      "68.45\n",
      "68.55\n",
      "68.65\n",
      "68.7\n",
      "68.65\n",
      "68.75\n",
      "68.7\n",
      "68.95\n",
      "68.95\n",
      "69.05\n",
      "69.05\n",
      "69.15\n",
      "69.25\n",
      "69.3\n",
      "69.4\n",
      "69.45\n",
      "69.55\n",
      "69.65\n",
      "69.75\n",
      "69.8\n",
      "69.75\n",
      "69.95\n",
      "69.95\n",
      "69.85\n",
      "70.0\n",
      "70.1\n",
      "70.15\n",
      "70.25\n",
      "70.35\n",
      "70.35\n",
      "70.35\n",
      "70.45\n",
      "70.5\n",
      "70.45\n",
      "70.45\n",
      "70.45\n",
      "70.6\n",
      "70.5\n",
      "70.5\n",
      "70.5\n",
      "70.65\n",
      "70.65\n",
      "70.7\n",
      "70.7\n",
      "70.7\n",
      "70.8\n",
      "70.75\n",
      "70.7\n",
      "70.7\n",
      "70.8\n",
      "70.95\n",
      "70.8\n",
      "70.8\n",
      "70.9\n",
      "70.95\n",
      "70.9\n",
      "70.95\n",
      "70.95\n",
      "71.1\n",
      "71.0\n",
      "71.05\n",
      "71.15\n",
      "71.0\n",
      "71.15\n",
      "71.1\n",
      "71.15\n",
      "71.25\n",
      "71.25\n",
      "71.3\n",
      "71.3\n",
      "71.35\n",
      "71.45\n",
      "71.5\n",
      "71.4\n",
      "71.65\n",
      "71.4\n",
      "71.4\n",
      "71.6\n",
      "71.5\n",
      "71.45\n",
      "71.55\n",
      "71.55\n",
      "71.55\n",
      "71.65\n",
      "71.65\n",
      "71.55\n",
      "71.7\n",
      "71.7\n",
      "71.7\n",
      "71.65\n",
      "71.75\n",
      "71.75\n",
      "71.8\n",
      "71.8\n",
      "71.95\n",
      "71.95\n",
      "71.95\n",
      "71.95\n",
      "72.05\n",
      "72.1\n",
      "72.05\n",
      "72.0\n",
      "72.05\n",
      "72.1\n",
      "72.1\n",
      "72.0\n",
      "72.05\n",
      "72.05\n",
      "72.1\n",
      "72.15\n",
      "72.2\n",
      "72.2\n",
      "72.2\n",
      "72.2\n",
      "72.2\n",
      "72.25\n",
      "72.25\n",
      "72.3\n",
      "72.3\n",
      "72.3\n",
      "72.2\n",
      "72.2\n",
      "72.35\n",
      "72.35\n",
      "72.35\n",
      "72.4\n",
      "72.35\n",
      "72.35\n",
      "72.4\n",
      "72.45\n",
      "72.4\n",
      "72.4\n",
      "72.45\n",
      "72.45\n",
      "72.45\n",
      "72.45\n",
      "72.35\n",
      "72.5\n",
      "72.5\n",
      "72.45\n",
      "72.45\n",
      "72.4\n",
      "72.45\n",
      "72.5\n",
      "72.45\n",
      "72.6\n",
      "72.55\n",
      "72.5\n",
      "72.45\n",
      "72.55\n",
      "72.6\n",
      "72.55\n",
      "72.6\n",
      "72.65\n",
      "72.75\n",
      "72.75\n",
      "72.75\n",
      "72.65\n",
      "72.8\n",
      "72.65\n",
      "72.75\n",
      "72.7\n",
      "72.95\n",
      "73.0\n",
      "72.85\n",
      "72.9\n",
      "72.95\n",
      "72.9\n",
      "73.0\n",
      "73.0\n",
      "72.9\n",
      "72.95\n",
      "72.9\n",
      "72.8\n",
      "72.8\n",
      "73.05\n",
      "72.75\n",
      "72.8\n",
      "72.85\n",
      "73.0\n",
      "73.0\n",
      "72.9\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-1*x))\n",
    "\n",
    "def lecun(x):\n",
    "    return 1.7159*np.tanh(2.0*x/3)\n",
    "\n",
    "def gradSigmoid(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "def gradLecun(x):\n",
    "    t = 2.0*x/3\n",
    "    return 1.7159*(1-t**2)\n",
    "\n",
    "def softmax(x):\n",
    "    x = np.exp(x)\n",
    "    x = x/x.sum(axis=1)[:, None]\n",
    "    return x\n",
    "    \n",
    "maxValAcc = 0\n",
    "lr = 0.00001\n",
    "trnAcc = []\n",
    "valAcc = []\n",
    "tstAcc = []\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "n_hid_1 = 100\n",
    "fan_in = 1.0#/np.sqrt(784)\n",
    "# print fan_in\n",
    "W1 = fan_in*(2*np.random.random((784,n_hid_1)) - 1)\n",
    "# print W1\n",
    "bias = fan_in*(2*np.random.random((n_hid_1)) - 1)\n",
    "fan_in_h = 1.0#/np.sqrt(n_hid_1)\n",
    "W2 = fan_in_h*(2*np.random.random((n_hid_1,10)) - 1)\n",
    "bias2 = fan_in_h*(2*np.random.random((10)) - 1)\n",
    "\n",
    "for j in xrange(300):\n",
    "    indices = np.random.choice(len(trnX), len(trnX))\n",
    "    tempY = trnY[indices]\n",
    "    tempX = trnX[indices]\n",
    "    A1 = np.dot(tempX, W1) + bias\n",
    "    l1 = sigmoid(A1)\n",
    "    \n",
    "    A2 = np.dot(l1, W2) + bias2\n",
    "    l2 = softmax(A2)\n",
    "\n",
    "    # Errors in output layer\n",
    "    d2 = (l2 - tempY)\n",
    "    dbias2 = np.sum(d2, axis = 0)\n",
    "        \n",
    "    # Delta of W2\n",
    "    dW2 = np.dot(l1.T, d2)\n",
    "\n",
    "    # Errors in 1st hidden layer\n",
    "    d1 = np.dot(d2, W2.T)*grad(l1)\n",
    "    dbias = np.sum(d1, axis = 0)    \n",
    "    \n",
    "    # Delta W2\n",
    "    dW1 = np.dot(tempX.T, d1)\n",
    "    \n",
    "    W2 -= lr*dW2\n",
    "    bias2 -= lr*dbias2\n",
    "    W1 -= lr*dW1\n",
    "    bias -= lr*dbias\n",
    "    \n",
    "    prediction = softmax(np.dot(lecun(np.dot(valX, W1)+bias ), W2)+bias2)\n",
    "    correct = [1 if a == b else 0 for (a, b) in zip(np.argmax(valY, axis = 1), np.argmax(prediction, axis = 1))]\n",
    "    valAcc.append(np.sum(correct)*100.0/len(valX))\n",
    "    \n",
    "    prediction = softmax(np.dot(lecun(np.dot(trnX, W1)+bias ), W2)+bias2)\n",
    "    correct = [1 if a == b else 0 for (a, b) in zip(np.argmax(trnY, axis = 1), np.argmax(prediction, axis = 1))]\n",
    "    trnAcc.append(np.sum(correct)*100.0/len(trnX))   \n",
    "    \n",
    "    prediction = softmax(np.dot(lecun(np.dot(tstX, W1)+bias ), W2)+bias2)\n",
    "    correct = [1 if a == b else 0 for (a, b) in zip(np.argmax(tstY, axis = 1), np.argmax(prediction, axis = 1))]\n",
    "    tstAcc.append(np.sum(correct)*100.0/len(tstX))   \n",
    "    \n",
    "#     if(valAcc[-1] > maxValAcc):\n",
    "    print tstAcc[-1]\n",
    "#         maxValAcc = valAcc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot([x+1 for x in range(len(trnAcc))], trnAcc, label = 'Training Accuracy')\n",
    "plt.plot([x+1 for x in range(len(valAcc))], valAcc, label = 'Validation Accuracy')\n",
    "plt.plot([x+1 for x in range(len(tstAcc))], tstAcc, label = 'Testing Accuracy')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc='lower right', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
