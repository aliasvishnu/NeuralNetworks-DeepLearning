{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mndata = MNIST('../HW1/Dataset')\n",
    "trainX = np.array(mndata.load_training()[0])[:50000]\n",
    "trainY = np.array(mndata.load_training()[1])[:50000]\n",
    "\n",
    "testX = np.array(mndata.load_testing()[0])[:1000]\n",
    "testY = np.array(mndata.load_testing()[1])[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valIndices = np.random.choice(len(trainX), 2000)\n",
    "nonValIndices = [x for x in range(len(trainX)) if x not in valIndices]\n",
    "\n",
    "valX = trainX[valIndices]\n",
    "valY = trainY[valIndices]\n",
    "\n",
    "trainX = trainX[nonValIndices]\n",
    "trainY = trainY[nonValIndices]\n",
    "\n",
    "testX = np.array(mndata.load_testing()[0])[:2000]\n",
    "testY = np.array(mndata.load_testing()[1])[:2000]\n",
    "\n",
    "def feat(data,i):\n",
    "    return data[i].tolist()\n",
    "\n",
    "def oneHot(clas, noOfClasses):\n",
    "    feat = np.zeros(noOfClasses)\n",
    "    feat[clas] = 1;\n",
    "    return feat\n",
    "\n",
    "trnX = np.array([feat(trainX,i) for i in range(trainX.shape[0])])/256.0\n",
    "trnY = np.array([oneHot(trainY[i], 10) for i in range(trainX.shape[0])])\n",
    "\n",
    "tstX = np.array([feat(testX,i) for i in range(testX.shape[0])])/256.0\n",
    "tstY = np.array([oneHot(testY[i], 10) for i in range(testX.shape[0])])\n",
    "\n",
    "valX = np.array([feat(valX,i) for i in range(valX.shape[0])])/256.0\n",
    "valY = np.array([oneHot(valY[i], 10) for i in range(valX.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.9\n",
      "16.15\n",
      "17.85\n",
      "23.75\n",
      "27.25\n",
      "29.3\n",
      "32.55\n",
      "34.8\n",
      "36.95\n",
      "38.8\n",
      "40.25\n",
      "41.9\n",
      "43.8\n",
      "45.4\n",
      "46.6\n",
      "48.15\n",
      "49.3\n",
      "50.95\n",
      "51.75\n",
      "52.55\n",
      "53.15\n",
      "54.35\n",
      "55.3\n",
      "55.85\n",
      "56.4\n",
      "57.35\n",
      "58.35\n",
      "58.9\n",
      "59.35\n",
      "60.4\n",
      "60.7\n",
      "61.65\n",
      "62.05\n",
      "62.55\n",
      "63.4\n",
      "63.75\n",
      "64.35\n",
      "64.45\n",
      "64.85\n",
      "65.15\n",
      "65.7\n",
      "66.05\n",
      "66.4\n",
      "66.5\n",
      "66.9\n",
      "67.05\n",
      "67.15\n",
      "67.65\n",
      "67.65\n",
      "68.15\n",
      "68.3\n",
      "68.9\n",
      "69.2\n",
      "69.3\n",
      "69.4\n",
      "69.5\n",
      "69.8\n",
      "70.05\n",
      "70.15\n",
      "70.2\n",
      "70.3\n",
      "70.45\n",
      "70.6\n",
      "70.95\n",
      "71.15\n",
      "71.45\n",
      "71.65\n",
      "71.95\n",
      "72.05\n",
      "72.3\n",
      "72.3\n",
      "72.45\n",
      "72.7\n",
      "72.95\n",
      "73.25\n",
      "73.3\n",
      "73.55\n",
      "73.6\n",
      "73.65\n",
      "73.85\n",
      "74.0\n",
      "74.35\n",
      "74.25\n",
      "74.65\n",
      "74.7\n",
      "74.8\n",
      "74.85\n",
      "74.95\n",
      "75.2\n",
      "75.1\n",
      "75.5\n",
      "75.4\n",
      "75.6\n",
      "75.65\n",
      "75.85\n",
      "75.95\n",
      "76.25\n",
      "76.4\n",
      "76.3\n",
      "76.25\n",
      "76.5\n",
      "76.8\n",
      "76.75\n",
      "76.85\n",
      "77.1\n",
      "77.2\n",
      "77.5\n",
      "77.55\n",
      "77.7\n",
      "77.95\n",
      "77.8\n",
      "78.15\n",
      "78.25\n",
      "78.4\n",
      "78.5\n",
      "78.45\n",
      "78.5\n",
      "78.7\n",
      "78.55\n",
      "78.65\n",
      "78.7\n",
      "78.8\n",
      "78.9\n",
      "78.85\n",
      "79.15\n",
      "79.1\n",
      "79.25\n",
      "79.15\n",
      "79.25\n",
      "79.15\n",
      "79.4\n",
      "79.55\n",
      "79.75\n",
      "79.55\n",
      "79.8\n",
      "79.9\n",
      "79.8\n",
      "79.95\n",
      "80.0\n",
      "80.05\n",
      "79.9\n",
      "80.15\n",
      "80.15\n",
      "80.2\n",
      "80.15\n",
      "80.25\n",
      "80.25\n",
      "80.3\n",
      "80.5\n",
      "80.55\n",
      "80.4\n",
      "80.45\n",
      "80.75\n",
      "80.65\n",
      "80.7\n",
      "80.85\n",
      "80.9\n",
      "81.1\n",
      "81.1\n",
      "80.95\n",
      "81.05\n",
      "81.1\n",
      "81.2\n",
      "81.25\n",
      "81.3\n",
      "81.3\n",
      "81.35\n",
      "81.35\n",
      "81.4\n",
      "81.5\n",
      "81.55\n",
      "81.5\n",
      "81.55\n",
      "81.55\n",
      "81.7\n",
      "81.85\n",
      "81.9\n",
      "81.85\n",
      "81.9\n",
      "82.1\n",
      "82.1\n",
      "82.15\n",
      "82.05\n",
      "82.05\n",
      "82.25\n",
      "82.35\n",
      "82.35\n",
      "82.2\n",
      "82.4\n",
      "82.4\n",
      "82.5\n",
      "82.5\n",
      "82.4\n",
      "82.5\n",
      "82.5\n",
      "82.55\n",
      "82.55\n",
      "82.6\n",
      "82.7\n",
      "82.65\n",
      "82.65\n",
      "82.75\n",
      "82.85\n",
      "82.8\n",
      "82.85\n",
      "82.9\n",
      "82.95\n",
      "83.0\n",
      "83.05\n",
      "82.95\n",
      "82.95\n",
      "83.25\n",
      "83.05\n",
      "83.15\n",
      "83.1\n",
      "83.2\n",
      "83.3\n",
      "83.45\n",
      "83.35\n",
      "83.35\n",
      "83.35\n",
      "83.35\n",
      "83.4\n",
      "83.35\n",
      "83.35\n",
      "83.45\n",
      "83.45\n",
      "83.5\n",
      "83.45\n",
      "83.4\n",
      "83.4\n",
      "83.4\n",
      "83.5\n",
      "83.55\n",
      "83.6\n",
      "83.65\n",
      "83.75\n",
      "83.75\n",
      "83.85\n",
      "83.8\n",
      "83.75\n",
      "83.95\n",
      "83.85\n",
      "84.0\n",
      "83.85\n",
      "83.85\n",
      "84.1\n",
      "84.0\n",
      "84.05\n",
      "84.0\n",
      "84.05\n",
      "83.9\n",
      "84.05\n",
      "84.1\n",
      "84.1\n",
      "84.05\n",
      "84.0\n",
      "84.05\n",
      "84.2\n",
      "84.25\n",
      "84.2\n",
      "84.2\n",
      "84.3\n",
      "84.3\n",
      "84.3\n",
      "84.4\n",
      "84.2\n",
      "84.3\n",
      "84.25\n",
      "84.2\n",
      "84.5\n",
      "84.5\n",
      "84.55\n",
      "84.55\n",
      "84.65\n",
      "84.5\n",
      "84.7\n",
      "84.6\n",
      "84.6\n",
      "84.75\n",
      "84.65\n",
      "84.75\n",
      "84.7\n",
      "84.75\n",
      "84.75\n",
      "84.8\n",
      "84.75\n",
      "84.8\n",
      "84.8\n",
      "84.9\n",
      "84.85\n",
      "84.8\n",
      "84.9\n",
      "84.85\n",
      "84.8\n",
      "84.8\n",
      "84.9\n",
      "84.95\n",
      "84.95\n",
      "84.95\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-1*x))\n",
    "\n",
    "def lecun(x):\n",
    "    return 1.7159*np.tanh(2.0*x/3)\n",
    "\n",
    "def grad(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "def gradLecun(x):\n",
    "    t = 2.0*x/3\n",
    "    return 1.7159*(1-t**2)\n",
    "\n",
    "def softmax(x):\n",
    "    x = np.exp(x)\n",
    "    x = x/x.sum(axis=1)[:, None]\n",
    "    return x\n",
    "    \n",
    "maxValAcc = 0\n",
    "lr = 0.00001\n",
    "trnAcc = []\n",
    "valAcc = []\n",
    "tstAcc = []\n",
    "\n",
    "# randomly initialize our weights with mean 0\n",
    "n_hid_1 = 100\n",
    "fan_in = 1.0#/np.sqrt(784)\n",
    "# print fan_in\n",
    "W1 = fan_in*(2*np.random.random((784,n_hid_1)) - 1)\n",
    "bias = fan_in*(2*np.random.random((n_hid_1)) - 1)\n",
    "fan_in_h = 1.0#/np.sqrt(n_hid_1)\n",
    "W2 = fan_in_h*(2*np.random.random((n_hid_1,10)) - 1)\n",
    "bias2 = fan_in_h*(2*np.random.random((10)) - 1)\n",
    "\n",
    "for j in xrange(300):\n",
    "    indices = np.random.choice(len(trnX), len(trnX))\n",
    "    tempY = trnY[indices]\n",
    "    tempX = trnX[indices]\n",
    "    A1 = np.dot(tempX, W1) + bias\n",
    "    l1 = sigmoid(A1)\n",
    "    \n",
    "    A2 = np.dot(l1, W2) + bias2\n",
    "    l2 = softmax(A2)\n",
    "\n",
    "    # Errors in output layer\n",
    "    d2 = (l2 - tempY)\n",
    "    dbias2 = np.sum(d2, axis = 0)\n",
    "        \n",
    "    # Delta of W2\n",
    "    dW2 = np.dot(l1.T, d2)\n",
    "\n",
    "    # Errors in 1st hidden layer\n",
    "    d1 = np.dot(d2, W2.T)*grad(l1)\n",
    "    dbias = np.sum(d1, axis = 0)    \n",
    "    \n",
    "    # Delta W2\n",
    "    dW1 = np.dot(tempX.T, d1)\n",
    "    \n",
    "    W2 -= lr*dW2\n",
    "    bias2 -= lr*dbias2\n",
    "    W1 -= lr*dW1\n",
    "    bias -= lr*dbias\n",
    "    \n",
    "    prediction = softmax(np.dot(sigmoid(np.dot(valX, W1)+bias), W2)+bias2)\n",
    "    correct = [1 if a == b else 0 for (a, b) in zip(np.argmax(valY, axis = 1), np.argmax(prediction, axis = 1))]\n",
    "    valAcc.append(np.sum(correct)*100.0/len(valX))\n",
    "    \n",
    "    prediction = softmax(np.dot(sigmoid(np.dot(trnX, W1)+bias), W2)+bias2)\n",
    "    correct = [1 if a == b else 0 for (a, b) in zip(np.argmax(trnY, axis = 1), np.argmax(prediction, axis = 1))]\n",
    "    trnAcc.append(np.sum(correct)*100.0/len(trnX))   \n",
    "    \n",
    "    prediction = softmax(np.dot(sigmoid(np.dot(tstX, W1)+bias), W2)+bias2)\n",
    "    correct = [1 if a == b else 0 for (a, b) in zip(np.argmax(tstY, axis = 1), np.argmax(prediction, axis = 1))]\n",
    "    tstAcc.append(np.sum(correct)*100.0/len(tstX))   \n",
    "    \n",
    "    print tstAcc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy =  84.95\n",
      "Final training accuracy = 86.6544523329\n",
      "Final validation accuracy =  86.35\n"
     ]
    }
   ],
   "source": [
    "print \"Final test accuracy = \", tstAcc[-1]\n",
    "print \"Final training accuracy =\", trnAcc[-1]\n",
    "print \"Final validation accuracy = \", valAcc[-1]\n",
    "plt.plot([x+1 for x in range(len(trnAcc))], trnAcc, label = 'Training Accuracy')\n",
    "plt.plot([x+1 for x in range(len(valAcc))], valAcc, label = 'Validation Accuracy')\n",
    "plt.plot([x+1 for x in range(len(tstAcc))], tstAcc, label = 'Testing Accuracy')\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc='lower right', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
